{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "826d3c5c",
   "metadata": {},
   "source": [
    "# what is PCA ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a552cc9",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a statistical procedure that uses an orthogonal transformation that converts a set of correlated variables to a set of uncorrelated variables. PCA is the most widely used tool in exploratory data analysis and in machine learning for predictive models. Moreover, PCA is an unsupervised statistical technique used to examine the interrelations among a set of variables. It is also known as a general factor analysis where regression determines a line of best fit.\n",
    "\n",
    "Principal Component Analysis (PCA) is one of the most commonly used unsupervised machine learning algorithms across a variety of applications: exploratory data analysis, dimensionality reduction, information compression, data de-noising, and plenty more.\n",
    "PCA allows us to go a step further and represent the data as linear combinations of principal components.\n",
    "\n",
    "dvantages of PCA:\n",
    "\n",
    "Easy to compute. PCA is based on linear algebra, which is computationally easy to solve by computers.\n",
    "\n",
    "Speeds up other machine learning algorithms. Machine learning algorithms converge faster when trained on principal components instead of the original dataset.\n",
    "\n",
    "Counteracts the issues of high-dimensional data. High-dimensional data causes regression-based algorithms to overfit easily. By using PCA beforehand to lower the dimensions of the training dataset, we prevent the predictive algorithms from overfitting.\n",
    "\n",
    "Disadvantages of PCA:\n",
    "\n",
    "Low interpretability of principal components. Principal components are linear combinations of the features from the original data, but they are not as easy to interpret. For example, it is difficult to tell which are the most important features in the dataset after computing principal components. \n",
    "\n",
    "The trade-off between information loss and dimensionality reduction. Although dimensionality reduction is useful, it comes at a cost. Information loss is a necessary part of PCA. Balancing the trade-off between dimensionality reduction and information loss is unfortunately a necessary compromise that we have to make when using PCA.\n",
    "To start PCA on the right foot, you will need to have the right tools that help you collect data from multiple sources and prepare it for machine learning models. Keboola covers all the steps, so you won't have to think about the infrastructure, only about the added-value your machine learning models will bring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15abe405",
   "metadata": {},
   "source": [
    "# non linear to linear transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e7dc16",
   "metadata": {},
   "source": [
    "Use logarithms to transform nonlinear data into a linear relationship so we can use least-squares regression methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7121d734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
